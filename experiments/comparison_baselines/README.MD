# Comparison RaFFM with baselines

## Baselines and Acknowledgement

We compare RaFFM with baseline:

**PriSM**: Overcoming Resource Constraints in Federated Learning: Large Models Can Be Trained with only Weak Clients

[PriSM Official Implementation](https://github.com/yuehniu/modeldecomp-fl/tree/master)

**PruneFL**: Model pruning enables efficient federated learning on edge devices

[PruneFL Official Implementation](https://github.com/jiangyuang/PruneFL)

### Acknowledgment

We thanks for the excellence of the baseline work and their great efforts. Despite these differences in objectives and model focus, **we recognize and value the foundational work in resource-efficient FL represented by PruneFL and PriSM**. These methods have provided essential insights and laid the groundwork for advancements like RaFFM, which aim to further enhance efficiency and scalability in FL.

## Experiment details
